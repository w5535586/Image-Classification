{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.vq import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from imutils import paths\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage import filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openfile(file):\n",
    "    f = open(file,\"r\", encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "\n",
    "    image_paths=[]  \n",
    "    image_classes=[]\n",
    "    for line in lines:\n",
    "        curLine=line.strip().split(\" \")\n",
    "        image_paths.append(curLine[0]) \n",
    "        image_classes.append(curLine[-1])\n",
    "\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    ims = []\n",
    "    print(\"loading images\")\n",
    "    for image_path in image_paths:\n",
    "        img  = cv2.imread(image_path)\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        img=cv2.resize(img,(64,64),interpolation=cv2.INTER_CUBIC)\n",
    "        ims.append(img)\n",
    "        print(str(time)+\"/\"+str(allt) +\"  \" + image_path, end=\"\\r\")\n",
    "        time = time +1\n",
    "    print(\"\\nDone!\")\n",
    "    return(ims,image_paths,image_classes)\n",
    "\n",
    "\n",
    "def siftfeature(image_paths,image_classes,ims):\n",
    "    sift= cv2.xfeatures2d.SIFT_create()\n",
    "    des_list = []\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    print(\"Calculating all the descriptors and keypoints...\")\n",
    "    for im in ims:\n",
    "        kp, des = sift.detectAndCompute(im,None)\n",
    "        if str(des) != 'None':\n",
    "            des_list.append((image_paths[time-1], des))       \n",
    "            print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "            time = time +1\n",
    "        else:\n",
    "            print(image_paths[time-1])\n",
    "            del image_classes[time-1]\n",
    "            del image_paths[time-1]\n",
    "    print(\"\\nDone!\")\n",
    "    print(\"image_classes:%d\",len(image_classes))\n",
    "    print(\"image_paths:%d\",len(image_paths))\n",
    "    \n",
    "    time=1\n",
    "    allt=len(des_list)\n",
    "    descriptors = des_list[0][1]\n",
    "    for image_path, descriptor in des_list[1:]:\n",
    "        descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "        time = time +1\n",
    "    \n",
    "    k = 128\n",
    "    voc, variance = kmeans(descriptors, k, 1)\n",
    "    \n",
    "    im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "    for i in range(len(image_paths)):\n",
    "        print(\"Calculating distance for image \"+str(i)+\"...\", end=\"\\r\")\n",
    "        words, distance = vq(des_list[i][1],voc)\n",
    "        for w in words:\n",
    "            im_features[i][w] += 1\n",
    "            \n",
    "    nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "    idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "    \n",
    "    stdSlr = StandardScaler().fit(im_features)\n",
    "    im_features = stdSlr.transform(im_features)\n",
    "    \n",
    "    return(im_features,image_classes)\n",
    "\n",
    "def lbpfeature(image_paths,image_classes,ims):\n",
    "    des_list = []\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    print(\"Calculating all the descriptors and keypoints...\")\n",
    "    for im in ims:\n",
    "        lbp=local_binary_pattern(im, 8, 1, 'default')\n",
    "        max_bins = int(lbp.max() + 1)\n",
    "        des, _ = np.histogram(lbp, normed=True, bins=max_bins, range=(0, max_bins))\n",
    "        if str(des) != 'None':\n",
    "            des_list.append(des)\n",
    "            print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "            time = time +1\n",
    "        else:\n",
    "            print(image_paths[time-1])\n",
    "            del image_classes[time-1]\n",
    "            del image_paths[time-1]\n",
    "    print(\"\\nDone!\")\n",
    "    \n",
    "    return(des_list,image_classes)\n",
    "\n",
    "def hogfeature(image_paths,image_classes,ims):\n",
    "    des_list = []\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    print(\"Calculating all the descriptors and keypoints...\")\n",
    "    for im in ims:\n",
    "        hog_image,des = hog(im, orientations=4, pixels_per_cell=(8,8),cells_per_block=(2, 2),block_norm= 'L2',visualise=True)\n",
    "        if str(des) != 'None':\n",
    "            des_list.append((image_paths[time-1], des))\n",
    "            print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "            time = time +1\n",
    "        else:\n",
    "            print(image_paths[time-1])\n",
    "            del image_classes[time-1]\n",
    "            del image_paths[time-1]\n",
    "    print(\"\\nDone!\")\n",
    "    print(\"image_classes:%d\",len(image_classes))\n",
    "    print(\"image_paths:%d\",len(image_paths))\n",
    "    \n",
    "    time=1\n",
    "    allt=len(des_list)\n",
    "    descriptors = des_list[0][1]\n",
    "    for image_path, descriptor in des_list[1:]:\n",
    "        descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "        time = time +1\n",
    "    \n",
    "    k = 128\n",
    "    voc, variance = kmeans(descriptors, k, 1)\n",
    "    \n",
    "    im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "    for i in range(len(image_paths)):\n",
    "        print(\"Calculating distance for image \"+str(i)+\"...\", end=\"\\r\")\n",
    "        words, distance = vq(des_list[i][1],voc)\n",
    "        for w in words:\n",
    "            im_features[i][w] += 1\n",
    "            \n",
    "    nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "    idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "    \n",
    "    stdSlr = StandardScaler().fit(im_features)\n",
    "    im_features = stdSlr.transform(im_features)\n",
    "    \n",
    "    return(im_features,image_classes)\n",
    "\n",
    "def sobelfeature(image_paths,image_classes,ims):\n",
    "    des_list = []\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    print(\"Calculating all the descriptors and keypoints...\")\n",
    "    for im in ims:\n",
    "        des=filters.sobel(im)\n",
    "        if str(des) != 'None':\n",
    "            des=des.flatten()\n",
    "            des_list.append(des)\n",
    "            print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "            time = time +1\n",
    "        else:\n",
    "            print(image_paths[time-1])\n",
    "            del image_classes[time-1]\n",
    "            del image_paths[time-1]\n",
    "    print(\"\\nDone!\")\n",
    "    \n",
    "    return(des_list,image_classes)\n",
    "    \n",
    "def siftSVC(im_features,image_classes,val_features):\n",
    "    print(\"start sift training model\")\n",
    "    siftclf = LinearSVC()\n",
    "    siftclf.fit(im_features, np.array(image_classes))\n",
    "    predictions=siftclf.predict(val_features)\n",
    "    \n",
    "    return(siftclf,predictions)\n",
    "\n",
    "def lbpSVC(im_features,image_classes,val_features):\n",
    "    print(\"start lbp training model\")\n",
    "    lbpclf = LinearSVC()\n",
    "    lbpclf.fit(im_features, np.array(image_classes))\n",
    "    predictions=lbpclf.predict(val_features)\n",
    "    \n",
    "    return(lbpclf,predictions)\n",
    "\n",
    "def hogSVC(im_features,image_classes,val_features):\n",
    "    print(\"start hog training model\")\n",
    "    hogclf = LinearSVC()\n",
    "    hogclf.fit(im_features, np.array(image_classes))\n",
    "    predictions=hogclf.predict(val_features)\n",
    "    \n",
    "    return(hogclf,predictions)\n",
    "\n",
    "def sobelSVC(im_features,image_classes,val_features):\n",
    "    print(\"start sobel training model\")\n",
    "    sobelclf = LinearSVC()\n",
    "    sobelclf.fit(im_features, np.array(image_classes))\n",
    "    predictions=sobelclf.predict(val_features)\n",
    "    \n",
    "    return(sobelclf,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start sift train\n",
      "loading images\n",
      "63325/63325  images/n02172182/n02172182_9821.JPEGGG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "images/n02115641/n02115641_232.JPEG\n",
      "images/n02117135/n02117135_987.JPEG\n",
      "images/n02123159/n02123159_2489.JPEG\n",
      "images/n02129165/n02129165_8099.JPEG\n",
      "images/n02134084/n02134084_17358.JPEG\n",
      "images/n02134084/n02134084_829.JPEG\n",
      "images/n02137549/n02137549_6460.JPEG\n",
      "images/n02165105/n02165105_510.JPEG\n",
      "63317/63325\n",
      "Done!\n",
      "image_classes:%d 63317\n",
      "image_paths:%d 63317\n",
      "start sift valstance for image 63316...\n",
      "loading images\n",
      "450/450  images/n02172182/n02172182_9982.JPEG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "450/450\n",
      "Done!\n",
      "image_classes:%d 450\n",
      "image_paths:%d 450\n",
      "start sift training modelimage 449...\n",
      "end sift train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#sift train\n",
    "print(\"start sift train\")\n",
    "ims=[]\n",
    "image_paths=[]\n",
    "image_classes=[]\n",
    "ims,image_paths,image_classes=openfile(\"train.txt\")\n",
    "sift_features,sift_image_classes=siftfeature(image_paths,image_classes,ims)\n",
    "\n",
    "#sift val\n",
    "print(\"start sift val\")\n",
    "imsval=[]\n",
    "image_pathsval=[]\n",
    "image_classesval=[]\n",
    "imsval,image_pathsval,image_classesval=openfile(\"val.txt\")\n",
    "val_sift_features,sift_fimage_classesval=siftfeature(image_pathsval,image_classesval,imsval)\n",
    "siftclf,siftpredictions=siftSVC(sift_features,sift_image_classes,val_sift_features)\n",
    "joblib.dump(siftclf, \"sift.pkl\", compress=3)\n",
    "print(\"end sift train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start lbp train\n",
      "loading images\n",
      "63325/63325  images/n02172182/n02172182_9821.JPEGGG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "43/63325\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel_launcher.py:81: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63325/63325\n",
      "Done!\n",
      "start lbp val\n",
      "loading images\n",
      "450/450  images/n02172182/n02172182_9982.JPEG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel_launcher.py:81: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450\n",
      "Done!\n",
      "start lbp training model\n",
      "end lbp train\n"
     ]
    }
   ],
   "source": [
    "#lbp train\n",
    "print(\"start lbp train\")\n",
    "ims=[]\n",
    "image_paths=[]\n",
    "image_classes=[]\n",
    "ims,image_paths,image_classes=openfile(\"train.txt\")\n",
    "lbp_features,lbp_image_classes=lbpfeature(image_paths,image_classes,ims)\n",
    "pca=PCA(n_components=128)\n",
    "lbp_features=pca.fit_transform(lbp_features)\n",
    "\n",
    "#lbp val\n",
    "print(\"start lbp val\")\n",
    "imsval=[]\n",
    "image_pathsval=[]\n",
    "image_classesval=[]\n",
    "imsval,image_pathsval,image_classesval=openfile(\"val.txt\")\n",
    "val_lbp_features,lbp_image_classesval=lbpfeature(image_pathsval,image_classesval,imsval)\n",
    "pca=PCA(n_components=128)\n",
    "val_lbp_features=pca.fit_transform(val_lbp_features)\n",
    "lbpclf,lbppredictions=lbpSVC(lbp_features,lbp_image_classes,val_lbp_features)\n",
    "joblib.dump(lbpclf, \"lbp.pkl\", compress=3)\n",
    "print(\"end lbp train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start hog train\n",
      "loading images\n",
      "63325/63325  images/n02172182/n02172182_9821.JPEGGG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "27/63325\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\skimage\\feature\\_hog.py:239: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63325/63325\n",
      "Done!\n",
      "image_classes:%d 63325\n",
      "image_paths:%d 63325\n",
      "start hog valistance for image 63324...\n",
      "loading images\n",
      "450/450  images/n02172182/n02172182_9982.JPEG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\skimage\\feature\\_hog.py:239: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450\n",
      "Done!\n",
      "image_classes:%d 450\n",
      "image_paths:%d 450\n",
      "start sift training modelimage 449...\n",
      "end hog train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#hog train\n",
    "print(\"start hog train\")\n",
    "ims=[]\n",
    "image_paths=[]\n",
    "image_classes=[]\n",
    "ims,image_paths,image_classes=openfile(\"train.txt\")\n",
    "hog_features,hog_image_classes=hogfeature(image_paths,image_classes,ims)\n",
    "\n",
    "#hog val\n",
    "print(\"start hog val\")\n",
    "imsval=[]\n",
    "image_pathsval=[]\n",
    "image_classesval=[]\n",
    "imsval,image_pathsval,image_classesval=openfile(\"val.txt\")\n",
    "val_hog_features,hog_image_classesval=hogfeature(image_pathsval,image_classesval,imsval)\n",
    "hogclf,hogpredictions=hogSVC(hog_features,hog_image_classes,val_hog_features)\n",
    "joblib.dump(hogclf, \"hog.pkl\", compress=3)\n",
    "print(\"end hog train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start sobel train\n",
      "loading images\n",
      "63325/63325  images/n02172182/n02172182_9821.JPEGGG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "63325/63325\n",
      "Done!\n",
      "start sobel val\n",
      "loading images\n",
      "450/450  images/n02172182/n02172182_9982.JPEG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "450/450\n",
      "Done!\n",
      "start sobel training model\n",
      "end sobel train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#sobel train\n",
    "print(\"start sobel train\")\n",
    "ims=[]\n",
    "image_paths=[]\n",
    "image_classes=[]\n",
    "ims,image_paths,image_classes=openfile(\"train.txt\")\n",
    "sobel_features,sobel_image_classes=sobelfeature(image_paths,image_classes,ims)\n",
    "pca=PCA(n_components=128)\n",
    "sobel_features=pca.fit_transform(sobel_features)\n",
    "\n",
    "#sobel val\n",
    "print(\"start sobel val\")\n",
    "imsval=[]\n",
    "image_pathsval=[]\n",
    "image_classesval=[]\n",
    "imsval,image_pathsval,image_classesval=openfile(\"val.txt\")\n",
    "val_sobel_features,sobel_image_classesval=sobelfeature(image_pathsval,image_classesval,imsval)\n",
    "pca=PCA(n_components=128)\n",
    "val_sobel_features=pca.fit_transform(val_sobel_features)\n",
    "sobelclf,sobelpredictions=sobelSVC(sobel_features,sobel_image_classes,val_sobel_features)\n",
    "joblib.dump(sobelclf, \"sobel.pkl\", compress=3)\n",
    "print(\"end sobel train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sift=accuracy_score(sift_fimage_classesval, siftpredictions)\n",
    "acc_lbp=accuracy_score(lbp_image_classesval, lbppredictions)\n",
    "acc_hog=accuracy_score(hog_image_classesval, hogpredictions)\n",
    "acc_sobel=accuracy_score(sobel_image_classesval, sobelpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_sift acc_lbp acc_hog acc_sobel\n",
      "0.011 0.0 0.033 0.027\n"
     ]
    }
   ],
   "source": [
    "print(round((acc_sift),3),round((acc_lbp)),round((acc_hog),3),round((acc_sobel),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump((clf, sift_image_paths, stdSlr, k, voc), \"sift.pkl\", compress=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

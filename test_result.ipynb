{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from imutils import paths\n",
    "import pickle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog\n",
    "from skimage.feature import local_binary_pattern\n",
    "from skimage import filters\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def openfile(file):\n",
    "    f = open(file,\"r\", encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "\n",
    "    image_paths=[]  \n",
    "    image_classes=[]\n",
    "    for line in lines:\n",
    "        curLine=line.strip().split(\" \")\n",
    "        image_paths.append(curLine[0]) \n",
    "        image_classes.append(curLine[-1])\n",
    "\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    ims = []\n",
    "    print(\"loading images\")\n",
    "    for image_path in image_paths:\n",
    "        img  = cv2.imread(image_path)\n",
    "        img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        img=cv2.resize(img,(64,64),interpolation=cv2.INTER_CUBIC)\n",
    "        ims.append(img)\n",
    "        print(str(time)+\"/\"+str(allt) +\"  \" + image_path, end=\"\\r\")\n",
    "        time = time +1\n",
    "    print(\"\\nDone!\")\n",
    "    return(ims,image_paths,image_classes)\n",
    "\n",
    "\n",
    "def siftfeature(image_paths,image_classes,ims):\n",
    "    sift= cv2.xfeatures2d.SIFT_create()\n",
    "    des_list = []\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    print(\"Calculating all the descriptors and keypoints...\")\n",
    "    for im in ims:\n",
    "        kp, des = sift.detectAndCompute(im,None)\n",
    "        if str(des) != 'None':\n",
    "            des_list.append((image_paths[time-1], des))       \n",
    "            print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "            time = time +1\n",
    "        else:\n",
    "            print(image_paths[time-1])\n",
    "            del image_classes[time-1]\n",
    "            del image_paths[time-1]\n",
    "    print(\"\\nDone!\")\n",
    "    print(\"image_classes:%d\",len(image_classes))\n",
    "    print(\"image_paths:%d\",len(image_paths))\n",
    "    \n",
    "    time=1\n",
    "    allt=len(des_list)\n",
    "    descriptors = des_list[0][1]\n",
    "    for image_path, descriptor in des_list[1:]:\n",
    "        descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "        time = time +1\n",
    "    \n",
    "    k = 128\n",
    "    voc, variance = kmeans(descriptors, k, 1)\n",
    "    \n",
    "    im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "    for i in range(len(image_paths)):\n",
    "        print(\"Calculating distance for image \"+str(i)+\"...\", end=\"\\r\")\n",
    "        words, distance = vq(des_list[i][1],voc)\n",
    "        for w in words:\n",
    "            im_features[i][w] += 1\n",
    "            \n",
    "    nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "    idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "    \n",
    "    stdSlr = StandardScaler().fit(im_features)\n",
    "    im_features = stdSlr.transform(im_features)\n",
    "    \n",
    "    return(im_features,image_classes)\n",
    "\n",
    "def lbpfeature(image_paths,image_classes,ims):\n",
    "    des_list = []\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    print(\"Calculating all the descriptors and keypoints...\")\n",
    "    for im in ims:\n",
    "        lbp=local_binary_pattern(im, 8, 1, 'default')\n",
    "        max_bins = int(lbp.max() + 1)\n",
    "        des, _ = np.histogram(lbp, normed=True, bins=max_bins, range=(0, max_bins))\n",
    "        if str(des) != 'None':\n",
    "            des_list.append(des)\n",
    "            print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "            time = time +1\n",
    "        else:\n",
    "            print(image_paths[time-1])\n",
    "            del image_classes[time-1]\n",
    "            del image_paths[time-1]\n",
    "    print(\"\\nDone!\")\n",
    "    \n",
    "    return(des_list,image_classes)\n",
    "\n",
    "def hogfeature(image_paths,image_classes,ims):\n",
    "    des_list = []\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    print(\"Calculating all the descriptors and keypoints...\")\n",
    "    for im in ims:\n",
    "        hog_image,des = hog(im, orientations=4, pixels_per_cell=(8,8),cells_per_block=(2, 2),block_norm= 'L2',visualise=True)\n",
    "        if str(des) != 'None':\n",
    "            des_list.append((image_paths[time-1], des))\n",
    "            print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "            time = time +1\n",
    "        else:\n",
    "            print(image_paths[time-1])\n",
    "            del image_classes[time-1]\n",
    "            del image_paths[time-1]\n",
    "    print(\"\\nDone!\")\n",
    "    print(\"image_classes:%d\",len(image_classes))\n",
    "    print(\"image_paths:%d\",len(image_paths))\n",
    "    \n",
    "    time=1\n",
    "    allt=len(des_list)\n",
    "    descriptors = des_list[0][1]\n",
    "    for image_path, descriptor in des_list[1:]:\n",
    "        descriptors = np.vstack((descriptors, descriptor))\n",
    "        print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "        time = time +1\n",
    "    \n",
    "    k = 128\n",
    "    voc, variance = kmeans(descriptors, k, 1)\n",
    "    \n",
    "    im_features = np.zeros((len(image_paths), k), \"float32\")\n",
    "    for i in range(len(image_paths)):\n",
    "        print(\"Calculating distance for image \"+str(i)+\"...\", end=\"\\r\")\n",
    "        words, distance = vq(des_list[i][1],voc)\n",
    "        for w in words:\n",
    "            im_features[i][w] += 1\n",
    "            \n",
    "    nbr_occurences = np.sum( (im_features > 0) * 1, axis = 0)\n",
    "    idf = np.array(np.log((1.0*len(image_paths)+1) / (1.0*nbr_occurences + 1)), 'float32')\n",
    "    \n",
    "    stdSlr = StandardScaler().fit(im_features)\n",
    "    im_features = stdSlr.transform(im_features)\n",
    "    \n",
    "    return(im_features,image_classes)\n",
    "\n",
    "def sobelfeature(image_paths,image_classes,ims):\n",
    "    des_list = []\n",
    "    time=1\n",
    "    allt=len(image_paths)\n",
    "    print(\"Calculating all the descriptors and keypoints...\")\n",
    "    for im in ims:\n",
    "        des=filters.sobel(im)\n",
    "        if str(des) != 'None':\n",
    "            des=des.flatten()\n",
    "            des_list.append(des)\n",
    "            print(str(time) + \"/\" + str(allt), end=\"\\r\")\n",
    "            time = time +1\n",
    "        else:\n",
    "            print(image_paths[time-1])\n",
    "            del image_classes[time-1]\n",
    "            del image_paths[time-1]\n",
    "    print(\"\\nDone!\")\n",
    "    \n",
    "    return(des_list,image_classes)\n",
    "\n",
    "def predicts(clf,im_feature):\n",
    "    predictions=clf.predict(im_feature)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "siftclf= joblib.load(\"sift.pkl\")\n",
    "lbpclf= joblib.load(\"lbp.pkl\")\n",
    "hogclf= joblib.load(\"hog.pkl\")\n",
    "sobelclf= joblib.load(\"sobel.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start sift test\n",
      "loading images\n",
      "450/450  images/n02172182/n02172182_981.JPEGG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "images/n02120079/n02120079_9750.JPEG\n",
      "449/450\n",
      "Done!\n",
      "image_classes:%d 449\n",
      "image_paths:%d 449\n",
      "Calculating distance for image 448...\r"
     ]
    }
   ],
   "source": [
    "print(\"start sift test\")\n",
    "imstest=[]\n",
    "image_pathstest=[]\n",
    "image_classestest=[]\n",
    "imstest,image_pathstest,image_classestest=openfile(\"test.txt\")\n",
    "test_sift_features,sift_image_classestest=siftfeature(image_pathstest,image_classestest,imstest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start lbp test\n",
      "loading images\n",
      "450/450  images/n02172182/n02172182_981.JPEGG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "18/450\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\ipykernel_launcher.py:81: VisibleDeprecationWarning: Passing `normed=True` on non-uniform bins has always been broken, and computes neither the probability density function nor the probability mass function. The result is only correct if the bins are uniform, when density=True will produce the same result anyway. The argument will be removed in a future version of numpy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"start lbp test\")\n",
    "imstest=[]\n",
    "image_pathstest=[]\n",
    "image_classestest=[]\n",
    "imstest,image_pathstest,image_classestest=openfile(\"test.txt\")\n",
    "test_lbp_features,lbp_image_classestest=lbpfeature(image_pathstest,image_classestest,imstest)\n",
    "pca=PCA(n_components=128)\n",
    "test_lbp_features=pca.fit_transform(test_lbp_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start hog test\n",
      "loading images\n",
      "450/450  images/n02172182/n02172182_981.JPEGG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "6/450\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\tensorflow1\\lib\\site-packages\\skimage\\feature\\_hog.py:239: skimage_deprecation: Argument `visualise` is deprecated and will be changed to `visualize` in v0.16\n",
      "  'be changed to `visualize` in v0.16', skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450\n",
      "Done!\n",
      "image_classes:%d 450\n",
      "image_paths:%d 450\n",
      "Calculating distance for image 449...\r"
     ]
    }
   ],
   "source": [
    "print(\"start hog test\")\n",
    "imstest=[]\n",
    "image_pathstest=[]\n",
    "image_classestest=[]\n",
    "imstest,image_pathstest,image_classestest=openfile(\"test.txt\")\n",
    "test_hog_features,hog_image_classestest=hogfeature(image_pathstest,image_classestest,imstest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start sobel test\n",
      "loading images\n",
      "450/450  images/n02172182/n02172182_981.JPEGG\n",
      "Done!\n",
      "Calculating all the descriptors and keypoints...\n",
      "450/450\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"start sobel test\")\n",
    "imstest=[]\n",
    "image_pathstest=[]\n",
    "image_classestest=[]\n",
    "imstest,image_pathstest,image_classestest=openfile(\"test.txt\")\n",
    "test_sobel_features,sobel_image_classestest=sobelfeature(image_pathstest,image_classestest,imstest)\n",
    "pca=PCA(n_components=128)\n",
    "test_sobel_features=pca.fit_transform(test_sobel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "siftpredictions=predicts(siftclf,test_sift_features)\n",
    "lbppredictions=predicts(lbpclf,test_lbp_features)\n",
    "hogpredictions=predicts(hogclf,test_hog_features)\n",
    "sobelpredictions=predicts(sobelclf,test_sobel_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_sift=round((accuracy_score(sift_image_classestest, siftpredictions)),3)\n",
    "acc_lbp=round((accuracy_score(lbp_image_classestest, lbppredictions)),3)\n",
    "acc_hog=round((accuracy_score(hog_image_classestest, hogpredictions)),3)\n",
    "acc_sobel=round((accuracy_score(sobel_image_classestest, sobelpredictions)),3)\n",
    "dic={\"sift\":acc_sift,\"lbp\":acc_lbp,\"hog\":acc_hog,\"sobel\":acc_sobel}\n",
    "id=[\"acc\"]\n",
    "df=pd.DataFrame(dic,index=id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       hog    lbp   sift  sobel\n",
      "acc  0.018  0.013  0.024  0.031\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
